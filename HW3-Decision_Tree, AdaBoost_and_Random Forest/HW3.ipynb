{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBlH4IaVJER-"
      },
      "source": [
        "## HW3: Decision Tree, AdaBoost and Random Forest\n",
        "In hw3, you need to implement decision tree, adaboost and random forest by using only numpy, then train your implemented model by the provided dataset. TA will use the on-hold test label to evaluate your model performance.\n",
        "\n",
        "Please note that only **NUMPY** can be used to implement your model, you will get no points by simply calling `sklearn.tree.DecisionTreeClassifier`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1ctWanXJESB"
      },
      "source": [
        "## Question 1\n",
        "Gini Index or Entropy is often used for measuring the “best” splitting of the data. Please compute the Entropy and Gini Index of provided data. Please use the formula from [page 5 of hw3 slides](https://docs.google.com/presentation/d/1kIe_-YZdemRMmr_3xDy-l0OS2EcLgDH7Uan14tlU5KE/edit#slide=id.gd542a5ff75_0_15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qKEBlYS3JESB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Copy and paste your implementations right here to check your result\n",
        "# (Of course you can add your classes not written here)\n",
        "def class_counts(sequence):\n",
        "    counts = {} # dictionary\n",
        "    for item in sequence:\n",
        "        label = item\n",
        "        if label not in counts: # 第一次算到\n",
        "            counts[label] = 0\n",
        "        counts[label] += 1\n",
        "    return counts\n",
        "\n",
        "def gini(sequence):\n",
        "    counts = class_counts(sequence)\n",
        "    gini = 1\n",
        "    for c in counts: # 跑過所有label\n",
        "        prob = counts[c] / len(sequence)\n",
        "        gini -= prob**2\n",
        "    return gini\n",
        "\n",
        "def entropy(sequence):\n",
        "    counts = class_counts(sequence)\n",
        "    entropy = 0\n",
        "    for c in counts: # 跑過所有label\n",
        "        prob = counts[c] / len(sequence)\n",
        "        entropy -= prob * np.log2(prob)\n",
        "    return entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sfciXR72JESC",
        "outputId": "cd00515c-aaaa-43aa-bfc8-19a6413c0082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gini of data is  0.4628099173553719\n",
            "Entropy of data is  0.9456603046006401\n"
          ]
        }
      ],
      "source": [
        "# 1 = class 1,\n",
        "# 2 = class 2\n",
        "data = np.array([1,2,1,1,1,1,2,2,1,1,2])\n",
        "print(\"Gini of data is \", gini(data))\n",
        "print(\"Entropy of data is \", entropy(data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWx_mc1QJESD"
      },
      "source": [
        "## Load data\n",
        "It is a binary classifiation dataset that classify if price is high or not for a cell phone, the label is stored in `price_range` column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAY5oyEtJESE",
        "outputId": "fb29d843-832c-48c2-8b39-30ac718010bc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1200, 21)\n",
            "(300, 21)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>battery_power</th>\n",
              "      <th>blue</th>\n",
              "      <th>clock_speed</th>\n",
              "      <th>dual_sim</th>\n",
              "      <th>fc</th>\n",
              "      <th>four_g</th>\n",
              "      <th>int_memory</th>\n",
              "      <th>m_dep</th>\n",
              "      <th>mobile_wt</th>\n",
              "      <th>n_cores</th>\n",
              "      <th>...</th>\n",
              "      <th>px_height</th>\n",
              "      <th>px_width</th>\n",
              "      <th>ram</th>\n",
              "      <th>sc_h</th>\n",
              "      <th>sc_w</th>\n",
              "      <th>talk_time</th>\n",
              "      <th>three_g</th>\n",
              "      <th>touch_screen</th>\n",
              "      <th>wifi</th>\n",
              "      <th>price_range</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1583</td>\n",
              "      <td>1</td>\n",
              "      <td>2.1</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>0.7</td>\n",
              "      <td>148</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>942</td>\n",
              "      <td>1651</td>\n",
              "      <td>1704</td>\n",
              "      <td>17</td>\n",
              "      <td>13</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>745</td>\n",
              "      <td>1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>35</td>\n",
              "      <td>0.8</td>\n",
              "      <td>102</td>\n",
              "      <td>8</td>\n",
              "      <td>...</td>\n",
              "      <td>89</td>\n",
              "      <td>1538</td>\n",
              "      <td>2459</td>\n",
              "      <td>14</td>\n",
              "      <td>1</td>\n",
              "      <td>16</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>832</td>\n",
              "      <td>0</td>\n",
              "      <td>0.7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>39</td>\n",
              "      <td>0.7</td>\n",
              "      <td>103</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>125</td>\n",
              "      <td>1504</td>\n",
              "      <td>1799</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>11</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1175</td>\n",
              "      <td>1</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0.3</td>\n",
              "      <td>164</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>873</td>\n",
              "      <td>1394</td>\n",
              "      <td>1944</td>\n",
              "      <td>9</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>695</td>\n",
              "      <td>0</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0.6</td>\n",
              "      <td>196</td>\n",
              "      <td>2</td>\n",
              "      <td>...</td>\n",
              "      <td>1649</td>\n",
              "      <td>1829</td>\n",
              "      <td>2855</td>\n",
              "      <td>16</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   battery_power  blue  clock_speed  dual_sim  fc  four_g  int_memory  m_dep  \\\n",
              "0           1583     1          2.1         1  11       0          14    0.7   \n",
              "1            745     1          0.6         1   5       0          35    0.8   \n",
              "2            832     0          0.7         1   2       1          39    0.7   \n",
              "3           1175     1          1.3         0   2       0          19    0.3   \n",
              "4            695     0          0.5         0  18       1          12    0.6   \n",
              "\n",
              "   mobile_wt  n_cores  ...  px_height  px_width   ram  sc_h  sc_w  talk_time  \\\n",
              "0        148        7  ...        942      1651  1704    17    13          2   \n",
              "1        102        8  ...         89      1538  2459    14     1         16   \n",
              "2        103        4  ...        125      1504  1799     5     2         11   \n",
              "3        164        7  ...        873      1394  1944     9     4          9   \n",
              "4        196        2  ...       1649      1829  2855    16    13          7   \n",
              "\n",
              "   three_g  touch_screen  wifi  price_range  \n",
              "0        1             0     1            1  \n",
              "1        1             1     0            0  \n",
              "2        1             0     1            0  \n",
              "3        1             1     0            0  \n",
              "4        1             1     1            1  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from csv import reader\n",
        "\n",
        "train_df = pd.read_csv('/Users/chunpei/ML/hw3/train.csv')\n",
        "val_df = pd.read_csv('/Users/chunpei/ML/hw3/val.csv')\n",
        "print(train_df.shape)\n",
        "print(val_df.shape)\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9yK3NTGJESE"
      },
      "source": [
        "## Question 2\n",
        "Implement the Decision Tree algorithm (CART, Classification and Regression Trees) and trained the model by the given arguments, and print the accuracy score on the validation data. You should implement two arguments for the Decision Tree algorithm\n",
        "1. **criterion**: The function to measure the quality of a split. Your model should support `gini` for the Gini impurity and `entropy` for the information gain. \n",
        "2. **max_depth**: The maximum depth of the tree. If `max_depth=None`, then nodes are expanded until all leaves are pure. `max_depth=1` equals to split data once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fH7MM_C8D514"
      },
      "outputs": [],
      "source": [
        "class Question: #column對應到第幾個feature，value代表分界的threshold\n",
        "    def __init__(self, column, value):\n",
        "        self.column = column\n",
        "        self.value = value\n",
        "\n",
        "class Leaf: #決定代表的class\n",
        "    def __init__(self, data):\n",
        "        content = [(value, key) for key, value in class_counts(data[:,-1]).items()]\n",
        "        self.predictions = max(content)[1]\n",
        "\n",
        "class Decision_Node: #record the question and two child nodes\n",
        "    def __init__(self, question, true_branch, false_branch, len_true_branch, len_false_branch):\n",
        "        self.question = question\n",
        "        self.true_branch = true_branch\n",
        "        self.false_branch = false_branch\n",
        "        self.len_true_branch = len_true_branch\n",
        "        self.len_false_branch = len_false_branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5IjOLUF1JESE"
      },
      "outputs": [],
      "source": [
        "class DecisionTree():\n",
        "    def __init__(self, criterion, max_depth):\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.max_features = -1\n",
        "        self.features_counting = np.zeros(20)\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "        row = x_data.shape[0]\n",
        "        col = x_data.shape[1] + 1 #加上lable\n",
        "        data = np.empty(shape=(row, col))\n",
        "        data.fill(0)\n",
        "        data[:, 0:(col-1)] = x_data\n",
        "        data[:, -1] = y_data #合併x_data, y_data成data\n",
        "        level = 0 # root depth = 0\n",
        "        self.features_counting = np.zeros(col - 1) # question3用到\n",
        "        self.mytree = self.build_tree(data, level)\n",
        "        \n",
        "    def build_tree(self, data, level):\n",
        "        gain, question = self.find_best_split(data) # 找到這一次best_split的gain跟question\n",
        "        if gain == 0 or level == self.max_depth: # gain = 0 or reach max_depth，Leaf node\n",
        "          return Leaf(data)\n",
        "        true_rows, false_rows = self.partition(data, question) # 把資料根據question分成兩堆\n",
        "        self.features_counting[question.column] += 1 # question3用到\n",
        "        # print(question.column, \" \", question.value, \" \", len(true_rows), \" \", len(false_rows))\n",
        "        true_branch = self.build_tree(true_rows, level+1) # build the true branch recursively\n",
        "        false_branch = self.build_tree(false_rows, level+1) # build the false branch recursively\n",
        "        return Decision_Node(question, true_branch, false_branch, len(true_rows), len(false_rows))\n",
        "\n",
        "    def find_best_split(self, data):\n",
        "      best_gain = 0\n",
        "      best_question = None\n",
        "      if self.criterion =='gini': # 根據不同criterion call function，calculate current value\n",
        "        cri_now = gini(data[:, -1])\n",
        "      elif self.criterion =='entropy':\n",
        "        cri_now = entropy(data[:,-1])\n",
        "      \n",
        "      if self.max_features != -1: # random forest case\n",
        "        feature_list = np.random.choice((data.shape[1]-1), self.max_features, replace=False) # produce feature list\n",
        "        feature_list = feature_list.tolist()\n",
        "      \n",
        "      for col in range(data.shape[1] - 1):  # for each feature column\n",
        "          if self.max_features != -1: # random forest case\n",
        "            if col not in feature_list: # 只做在feature_list內的\n",
        "              continue\n",
        "          values = set([data[i, col] for i in range(data.shape[0])]) # unique values in this feature column\n",
        "          for val in values:\n",
        "              question = Question(col, val)\n",
        "              true_rows, false_rows = self.partition(data, question) # 把資料根據question分成兩堆，測試看看question選得好不好\n",
        "              if len(true_rows) == 0 or len(false_rows) == 0: # can't divide -> skip\n",
        "                  continue\n",
        "              p = float(len(true_rows) / (len(true_rows) + len(false_rows)))\n",
        "              if self.criterion =='gini':\n",
        "                gain = cri_now - p * gini(true_rows[:, -1]) - (1 - p) * gini(false_rows[:, -1]) # calculate the information gain\n",
        "              elif self.criterion =='entropy':\n",
        "                gain = cri_now - p * entropy(true_rows[:, -1]) - (1 - p) * entropy(false_rows[:, -1])\n",
        "              if gain > best_gain:\n",
        "                  best_gain, best_question = gain, question\n",
        "      return best_gain, best_question\n",
        "\n",
        "    def partition(self, data, question):\n",
        "      true_rows, false_rows = [], []\n",
        "      for i in range(data.shape[0]): # 所有資料根據question做分類\n",
        "          if data[i, question.column] >= question.value:\n",
        "              true_rows.append(data[i, :])\n",
        "          else:\n",
        "              false_rows.append(data[i, :])\n",
        "      true_rows = np.array(true_rows)\n",
        "      false_rows = np.array(false_rows)\n",
        "      return true_rows, false_rows\n",
        "    \n",
        "    def predict(self, data):\n",
        "        if isinstance(data, pd.DataFrame): data = data.values\n",
        "        predit_list = []\n",
        "        for i in range(data.shape[0]):\n",
        "          pred = self.classify(data, i, self.mytree) # 每筆資料的classify結果\n",
        "          predit_list.append(pred)\n",
        "        predit_list = np.array(predit_list)\n",
        "        return predit_list\n",
        "\n",
        "    def classify(self, data, i, node):\n",
        "        if isinstance(node, Leaf): # reach a leaf\n",
        "            return node.predictions\n",
        "        if data[i, node.question.column] >= node.question.value:\n",
        "            return self.classify(data, i, node.true_branch) # recursive直到找到結果\n",
        "        else:\n",
        "            return self.classify(data, i, node.false_branch)\n",
        "\n",
        "    def accuracy(self, data):\n",
        "        t = 0\n",
        "        f = 0\n",
        "        for i in range(data.shape[0]):\n",
        "            if data[i, -1] == self.classify(data, i, self.mytree): # correct\n",
        "                t += 1\n",
        "            else:\n",
        "                f += 1\n",
        "        return t / (t + f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBhHqq7JJESF"
      },
      "source": [
        "### Question 2.1\n",
        "Using `criterion=gini`, showing the accuracy score of validation data by `max_depth=3` and `max_depth=10`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVwnLU0oJESF",
        "outputId": "b87bc438-dbf0-47da-ae90-f3affd3a90e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.92\n",
            "0.93\n"
          ]
        }
      ],
      "source": [
        "clf_depth3 = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_depth3.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_depth3.accuracy(val_df.values))\n",
        "\n",
        "clf_depth10 = DecisionTree(criterion='gini', max_depth=10)\n",
        "clf_depth10.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_depth10.accuracy(val_df.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCIYJsLqJESF"
      },
      "source": [
        "### Question 2.2\n",
        "Using `max_depth=3`, showing the accuracy score of validation data by `criterion=gini` and `criterion=entropy`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Lvnm6wxJESF",
        "outputId": "5c8a9ab6-0c76-4eb1-970d-b4c7f48924f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.92\n",
            "0.9333333333333333\n"
          ]
        }
      ],
      "source": [
        "clf_gini = DecisionTree(criterion='gini', max_depth=3)\n",
        "clf_gini.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_gini.accuracy(val_df.values))\n",
        "\n",
        "clf_entropy = DecisionTree(criterion='entropy', max_depth=3)\n",
        "clf_entropy.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_entropy.accuracy(val_df.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8NGYc8EJESF"
      },
      "source": [
        "- Note: Your decisition tree scores should over **0.9**. It may suffer from overfitting, if so, you can tune the hyperparameter such as `max_depth`\n",
        "- Note: You should get the same results when re-building the model with the same arguments,  no need to prune the trees\n",
        "- Hint: You can use the recursive method to build the nodes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34sUhW-bJESF"
      },
      "source": [
        "## Question 3\n",
        "Plot the [feature importance](https://sefiks.com/2020/04/06/feature-importance-in-decision-trees/) of your Decision Tree model. You can get the feature importance by counting the feature used for splitting data.\n",
        "\n",
        "- You can simply plot the **counts of feature used** for building tree without normalize the importance. Take the figure below as example, outlook feature has been used for splitting for almost 50 times. Therefore, it has the largest importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlyRg6y1myz8",
        "outputId": "5ef8527b-5fec-4351-f029-f525f751dae0"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "label = ['battery_power','blue','clock_speed','dual_sim','fc','four_g','int_memory','m_dep','mobile_wt','n_cores','pc','px_height','px_width','ram','sc_h','sc_w','talk_time','three_g','touch_screen','wifi']\n",
        "feature_count = []\n",
        "final_label = [] # 有被用到的再印\n",
        "for i in range(train_df.shape[1]-1):\n",
        "    if clf_depth10.features_counting[i] != 0: # 有用到這個feature\n",
        "        feature_count.append(clf_depth10.features_counting[i])\n",
        "        final_label.append(label[i])\n",
        "plt.barh(final_label, feature_count, tick_label=final_label, height=0.5)\n",
        "x_ticks=np.arange(0,20,1)\n",
        "plt.xticks(x_ticks)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFdiX0tsJESG"
      },
      "source": [
        "## Question 4\n",
        "implement the AdaBooest algorithm by using the CART you just implemented from question 2 as base learner. You should implement one arguments for the AdaBooest.\n",
        "1. **n_estimators**: The maximum number of estimators at which boosting is terminated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Fo94fJPzJESG"
      },
      "outputs": [],
      "source": [
        "class AdaBoost():\n",
        "    def __init__(self, n_estimators):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.dtrees = []\n",
        "        self.mytrees = []\n",
        "        self.criterion = 'gini'\n",
        "        self.max_depth = 1\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "        row = x_data.shape[0]\n",
        "        col = x_data.shape[1] + 1 #加上lable\n",
        "        data = np.empty(shape=(row, col))\n",
        "        data.fill(0)\n",
        "        data[:, 0:(col-1)] = x_data\n",
        "        data[:, -1] = y_data #合併x_data, y_data成data\n",
        "        self.build_tree(data)\n",
        "\n",
        "    def build_tree(self, o_data):\n",
        "        w = np.full(o_data.shape[0], (1 / o_data.shape[0]))\n",
        "        for _ in range(self.n_estimators): # for each classifiers\n",
        "          index = np.random.choice(o_data.shape[0], o_data.shape[0], replace=True, p=w) # 根據weight隨機sample data\n",
        "          index = index.tolist()\n",
        "          data = np.empty(shape=(o_data.shape[0],o_data.shape[1]))\n",
        "          data.fill(0)\n",
        "          for i in range(o_data.shape[0]):\n",
        "            data[i] = o_data[index[i]] # update data\n",
        "\n",
        "          dtree = DecisionTree(criterion=self.criterion, max_depth=self.max_depth) # find the best stump (using updated data)\n",
        "          mytree = dtree.build_tree(data, 0)\n",
        "\n",
        "          total_error = 0\n",
        "          for i in range(o_data.shape[0]): # for each original data\n",
        "            if o_data[i, -1] != dtree.classify(o_data, i, mytree): # incorrect\n",
        "              total_error += w[i]\n",
        "          mytree.alpha = (1/2) * (np.log((1 - total_error) / total_error)) # calculate alpha\n",
        "\n",
        "          for i in range(o_data.shape[0]):\n",
        "            if o_data[i, -1] != dtree.classify(o_data, i, mytree): # incorrect -> +alpha\n",
        "              w[i] *= np.exp(mytree.alpha)\n",
        "            else:\n",
        "              w[i] *= np.exp(-(mytree.alpha)) # correct -> -alpha\n",
        "          w /= sum(w) # normalize\n",
        "\n",
        "          # save\n",
        "          self.dtrees.append(dtree)\n",
        "          self.mytrees.append(mytree)\n",
        "\n",
        "    def predict(self, data):\n",
        "        if isinstance(data, pd.DataFrame): data = data.values\n",
        "        predit_list = []\n",
        "        for i in range(data.shape[0]):\n",
        "          result = 0\n",
        "          for j in range(self.n_estimators):\n",
        "            if self.dtrees[j].classify(data, i, self.mytrees[j]) == 0: # classify結果為0(相較於adaboost的-1)\n",
        "              result -= self.mytrees[j].alpha\n",
        "            else:\n",
        "              result += self.mytrees[j].alpha\n",
        "          if result > 0: # ababoost class1，相當於data的class1\n",
        "            pred = 1\n",
        "          else: # ababoost class-1，相當於data的class0\n",
        "            pred = 0\n",
        "          predit_list.append(pred)\n",
        "        predit_list = np.array(predit_list)\n",
        "        return predit_list\n",
        "\n",
        "    def accuracy(self, data):\n",
        "      t = 0\n",
        "      f = 0\n",
        "      predit_list = self.predict(data)\n",
        "      for i in range(data.shape[0]):\n",
        "        if predit_list[i] == data[i, -1]:\n",
        "          t += 1\n",
        "        else:\n",
        "          f += 1\n",
        "      return t / (t + f)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lat2NMgJESG"
      },
      "source": [
        "### Question 4.1\n",
        "Show the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gikbj2jiJESG",
        "outputId": "074f45a0-792e-48ca-8113-7a54afa1a54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9466666666666667\n",
            "0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "clf10 = AdaBoost(n_estimators=10)\n",
        "clf10.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf10.accuracy(val_df.values))\n",
        "\n",
        "clf100 = AdaBoost(n_estimators=100)\n",
        "clf100.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf100.accuracy(val_df.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jXfS5vSPJESG"
      },
      "source": [
        "## Question 5\n",
        "implement the Random Forest algorithm by using the CART you just implemented from question 2. You should implement three arguments for the Random Forest.\n",
        "\n",
        "1. **n_estimators**: The number of trees in the forest. \n",
        "2. **max_features**: The number of random select features to consider when looking for the best split\n",
        "3. **bootstrap**: Whether bootstrap samples are used when building tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-e-h7rDTJESG"
      },
      "outputs": [],
      "source": [
        "class RandomForest():\n",
        "    def __init__(self, n_estimators, max_features, boostrap=True, criterion='gini', max_depth=None):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.max_features = int(np.around(max_features))\n",
        "        self.boostrap = boostrap\n",
        "        self.criterion = criterion\n",
        "        self.max_depth = max_depth\n",
        "        self.dtrees = []\n",
        "        self.mytrees = []\n",
        "\n",
        "    def fit(self, x_data, y_data):\n",
        "        row = x_data.shape[0]\n",
        "        col = x_data.shape[1] + 1 #加上lable\n",
        "        data = np.empty(shape=(row, col))\n",
        "        data.fill(0)\n",
        "        data[:, 0:(col-1)] = x_data\n",
        "        data[:, -1] = y_data #合併x_data, y_data成data\n",
        "        self.build_tree(data)\n",
        "\n",
        "    def build_tree(self, o_data):\n",
        "        total_row = o_data.shape[0]\n",
        "        total_col = o_data.shape[1] - 1\n",
        "        for _ in range(self.n_estimators): # for each classifiers\n",
        "          if self.boostrap == True:\n",
        "            row_index = np.random.choice(total_row, total_row, replace=True) # 隨機sample data (uniform distributed)\n",
        "            row_index = row_index.tolist()\n",
        "            data = np.empty(shape=(total_row, total_col+1))\n",
        "            data.fill(0)\n",
        "            for i in range(total_row):\n",
        "              data[i] = o_data[row_index[i]] # update data\n",
        "          else:\n",
        "            data = o_data\n",
        "          dtree = DecisionTree(criterion=self.criterion, max_depth=self.max_depth)\n",
        "          dtree.max_features = self.max_features # 設定max_features\n",
        "          mytree = dtree.build_tree(data, 0)\n",
        "          self.dtrees.append(dtree)\n",
        "          self.mytrees.append(mytree)\n",
        "\n",
        "    def predict(self, data):\n",
        "        if isinstance(data, pd.DataFrame): data = data.values\n",
        "        predit_list = []\n",
        "        for i in range(data.shape[0]):\n",
        "          pred0 = 0\n",
        "          pred1 = 0\n",
        "          for j in range(self.n_estimators):\n",
        "            if self.dtrees[j].classify(data, i, self.mytrees[j]) == 0: # classify結果為0\n",
        "              pred0 += 1\n",
        "            else:\n",
        "              pred1 +=1\n",
        "          if pred0 > pred1: # majority vote\n",
        "            pred = 0\n",
        "          else:\n",
        "            pred = 1\n",
        "          predit_list.append(pred)\n",
        "        predit_list = np.array(predit_list)\n",
        "        return predit_list\n",
        "\n",
        "    def accuracy(self, data):\n",
        "      t = 0\n",
        "      f = 0\n",
        "      predit_list = self.predict(data)\n",
        "      for i in range(data.shape[0]):\n",
        "        if predit_list[i] == data[i, -1]:\n",
        "          t += 1\n",
        "        else:\n",
        "          f += 1\n",
        "      return t / (t + f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-a9_3oUJESG"
      },
      "source": [
        "### Question 5.1\n",
        "Using `criterion=gini`, `max_depth=None`, `max_features=sqrt(n_features)`, showing the accuracy score of validation data by `n_estimators=10` and `n_estimators=100`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-n9V0nIJESG",
        "outputId": "2d301d02-3a49-412c-a7d6-749c4e460d3f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9466666666666667\n",
            "0.9466666666666667\n"
          ]
        }
      ],
      "source": [
        "clf_10tree = RandomForest(n_estimators=10, max_features=np.sqrt(train_df.values.shape[1]-1))\n",
        "clf_10tree.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_10tree.accuracy(val_df.values))\n",
        "\n",
        "clf_100tree = RandomForest(n_estimators=100, max_features=np.sqrt(train_df.values.shape[1]-1))\n",
        "clf_100tree.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_100tree.accuracy(val_df.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPw6pWDMJESH"
      },
      "source": [
        "### Question 5.2\n",
        "Using `criterion=gini`, `max_depth=None`, `n_estimators=10`, showing the accuracy score of validation data by `max_features=sqrt(n_features)` and `max_features=n_features`, respectively.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibyp9eD1JESH",
        "outputId": "c933a777-e11c-4fe0-89bf-018c421510dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9566666666666667\n",
            "0.96\n"
          ]
        }
      ],
      "source": [
        "clf_random_features = RandomForest(n_estimators=10, max_features=np.sqrt(train_df.values.shape[1]-1))\n",
        "clf_random_features.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_random_features.accuracy(val_df.values))\n",
        "\n",
        "clf_all_features = RandomForest(n_estimators=10, max_features=(train_df.values.shape[1]-1))\n",
        "clf_all_features.fit(train_df.values[:, 0:(train_df.shape[1]-1)], train_df.values[:, -1])\n",
        "print(clf_all_features.accuracy(val_df.values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAqOgJn1JESH"
      },
      "source": [
        "- Note: Use majority votes to get the final prediction, you may get slightly different results when re-building the random forest model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vxgrqog_JESH"
      },
      "source": [
        "## Question 6. Train and tune your model on a real-world dataset\n",
        "Try you best to get higher accuracy score of your model. After parameter tuning, you can train your model on the full dataset (train + val).\n",
        "- Feature engineering\n",
        "- Hyperparameter tuning\n",
        "- Implement any other ensemble methods, such as gradient boosting. Please note that you **can not** call any package. Also, only ensemble method can be used. Neural network method is not allowed to used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "npis7PDnJESH"
      },
      "outputs": [],
      "source": [
        "def train_your_model(data):\n",
        "    model = AdaBoost(n_estimators=100)\n",
        "    model.criterion = 'gini'\n",
        "    model.max_depth = 1\n",
        "    model.fit(data.values[:, 0:(data.shape[1]-1)], data.values[:, -1])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "VBOeuU_fJESH"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('/Users/chunpei/ML/hw3/train.csv')\n",
        "val_df = pd.read_csv('/Users/chunpei/ML/hw3/val.csv')\n",
        "trainval_df = pd.concat([train_df, val_df])\n",
        "my_model = train_your_model(trainval_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "oVs1Fv1cJESH"
      },
      "outputs": [],
      "source": [
        "x_test = pd.read_csv('/Users/chunpei/ML/hw3/x_test.csv')\n",
        "y_pred = my_model.predict(x_test)\n",
        "\n",
        "\n",
        "with open('model.pickle', 'wb') as pkl_file:\n",
        "    pickle.dump(my_model, pkl_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "np.save(\"y_pred.npy\", y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "IAbWlNm1JESH"
      },
      "outputs": [],
      "source": [
        "assert y_pred.shape == (500, )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHGxXlwKJESH"
      },
      "source": [
        "## Supplementary\n",
        "If you have trouble to implement this homework, TA strongly recommend watching [this video](https://www.youtube.com/watch?v=LDRbO9a6XPU), which explains Decision Tree model clearly. But don't copy code from any resources, try to finish this homework by yourself! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5MQYdwmJESH"
      },
      "source": [
        "### DO NOT MODIFY CODE BELOW"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dpes-uG9JESH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMD2FTh_FPoj"
      },
      "outputs": [],
      "source": [
        "y_test = pd.read_csv('y_test.csv')['price_range'].values\n",
        "\n",
        "print('Test-set accuarcy score: ', accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePRSbt4UJESI",
        "outputId": "14722d4e-37fb-43d7-a700-480e3261483c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "*** We will check your result for Question 3 manually *** (5 points)\n",
            "*** We will check your result for Question 6 manually *** (20 points)\n",
            "Approximate score range: 45.0 ~ 70.0\n",
            "*** This score is only for reference ***\n"
          ]
        }
      ],
      "source": [
        "def discrete_checker(score, thres, clf, name, x_train, y_train, x_test, y_test):\n",
        "    clf.fit(x_train, y_train)\n",
        "    y_pred = clf.predict(x_test)\n",
        "    if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "        return score\n",
        "    else:\n",
        "        print(f\"{name} failed\")\n",
        "        return 0\n",
        "\n",
        "\n",
        "def patient_checker(score, thres, CLS, kwargs, name,\n",
        "                    x_train, y_train, x_test, y_test, patient=10):\n",
        "    while patient > 0:\n",
        "        patient -= 1\n",
        "        clf = CLS(**kwargs)\n",
        "        clf.fit(x_train, y_train)\n",
        "        y_pred = clf.predict(x_test)\n",
        "        if accuracy_score(y_test, y_pred) - thres >= 0:\n",
        "            return score\n",
        "    print(f\"{name} failed\")\n",
        "    print(\"Considering the randomness, we will check it manually\")\n",
        "    return 0\n",
        "\n",
        "\n",
        "def load_dataset():\n",
        "    file_url = \"http://storage.googleapis.com/download.tensorflow.org/data/abalone_train.csv\"\n",
        "    df = pd.read_csv(\n",
        "        file_url,\n",
        "        names=[\"Length\", \"Diameter\", \"Height\", \"Whole weight\", \"Shucked weight\",\n",
        "               \"Viscera weight\", \"Shell weight\", \"Age\"]\n",
        "    )\n",
        "\n",
        "    df['Target'] = (df[\"Age\"] > 15).astype(int)\n",
        "    df = df.drop(labels=[\"Age\"], axis=\"columns\")\n",
        "\n",
        "    train_idx = range(0, len(df), 10)\n",
        "    test_idx = range(1, len(df), 20)\n",
        "\n",
        "    train_df = df.iloc[train_idx]\n",
        "    test_df = df.iloc[test_idx]\n",
        "\n",
        "    x_train = train_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    feature_names = x_train.columns.values\n",
        "    x_train = x_train.values\n",
        "    y_train = train_df['Target'].values\n",
        "\n",
        "    x_test = test_df.drop(labels=[\"Target\"], axis=\"columns\")\n",
        "    x_test = x_test.values\n",
        "    y_test = test_df['Target'].values\n",
        "    return x_train, y_train, x_test, y_test, feature_names\n",
        "\n",
        "\n",
        "score = 0\n",
        "\n",
        "data = np.array([1, 2])\n",
        "if abs(gini(data) - 0.5) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"gini test failed\")\n",
        "\n",
        "if abs(entropy(data) - 1) < 1e-4:\n",
        "    score += 2.5\n",
        "else:\n",
        "    print(\"entropy test failed\")\n",
        "\n",
        "x_train, y_train, x_test, y_test, feature_names = load_dataset()\n",
        "\n",
        "score += discrete_checker(5, 0.9337,\n",
        "                          DecisionTree(criterion='gini', max_depth=3),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9036,\n",
        "                          DecisionTree(criterion='gini', max_depth=10),\n",
        "                          \"DecisionTree(criterion='gini', max_depth=10)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "score += discrete_checker(2.5, 0.9096,\n",
        "                          DecisionTree(criterion='entropy', max_depth=3),\n",
        "                          \"DecisionTree(criterion='entropy', max_depth=3)\",\n",
        "                          x_train, y_train, x_test, y_test\n",
        "                          )\n",
        "\n",
        "print(\"*** We will check your result for Question 3 manually *** (5 points)\")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.91, AdaBoost, {\"n_estimators\": 10},\n",
        "    \"AdaBoost(n_estimators=10)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    7.5, 0.87, AdaBoost, {\"n_estimators\": 100},\n",
        "    \"AdaBoost(n_estimators=100)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=10, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.91, RandomForest,\n",
        "    {\"n_estimators\": 100, \"max_features\": np.sqrt(x_train.shape[1])},\n",
        "    \"RandomForest(n_estimators=100, max_features=sqrt(n_features))\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "score += patient_checker(\n",
        "    5, 0.92, RandomForest,\n",
        "    {\"n_estimators\": 10, \"max_features\": x_train.shape[1]},\n",
        "    \"RandomForest(n_estimators=10, max_features=n_features)\",\n",
        "    x_train, y_train, x_test, y_test\n",
        ")\n",
        "\n",
        "print(\"*** We will check your result for Question 6 manually *** (20 points)\")\n",
        "print(\"Approximate score range:\", score, \"~\", score + 25)\n",
        "print(\"*** This score is only for reference ***\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HWx_mc1QJESD",
        "34sUhW-bJESF",
        "EFdiX0tsJESG"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
